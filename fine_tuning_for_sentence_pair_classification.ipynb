{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuLR0BMJhIub"
      },
      "source": [
        "# Fine-tuning Pre-trained Models for Sentence-pair Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZIHhCYNhN-H"
      },
      "source": [
        "## 1. Introduction \n",
        "\n",
        "In this programming exercise, you will fine-tune a publicly released pre-trained model for the **sentence-pair classification** task using PyTorch.\n",
        "\n",
        "The dataset used is *Microsoft Research Paraphrase Corpus* (MRPC): you have two sentences and you want to predict if one sentence is the paraphrase of the other one. The evaluation metrics are F1 and accuracy.\n",
        "\n",
        "What you are required to do in this programming exercise is to implement a binary classifier in **Section 3** to perform the classification. Please see the comments in **Section 3** for more details about the implementation. \n",
        "\n",
        "Once you complete the implementation, you will train the model and test it on the MRPC test set. The training time will be about 10 minutes if you are using the free GPU from [Colab](https://colab.research.google.com/?utm_source=scs-index).\n",
        "\n",
        "You will get 80% marks on this question once you correctly implement the model and get the results (accuracy > 0.65 and F1 > 0.75). To get a higher mark in this programming exercise, you are encouraged to do the following things to get a better result:\n",
        "\n",
        "\n",
        "*   Tuning the parameters listed in **Section 3**;\n",
        "*   Implementing a complex classifier, e.g., MLP.\n",
        "\n",
        "You will get full marks on this question once the test set performance exceeds the threshold (accuracy > 0.8 or F1 > 0.85).\n",
        "\n",
        "**Note**: Do not modify the code in other parts of this notebook and Make sure that you run cells in this notebook sequentially.\n",
        "\n",
        "For submission, please submit the whole notebook to the Blackboard. Make sure that your submitted notebook contains the outputs of all cells.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIv7rF4V6lyE"
      },
      "source": [
        "## 2. Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42-lJ1u9IHT6"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE_TpNaSZQ5n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup, AutoConfig\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF8IWdEowPP-"
      },
      "outputs": [],
      "source": [
        "# Check that we are using 100% of GPU memory footprint support libraries/code\n",
        "# from https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip -q install gputil\n",
        "!pip -q install psutil\n",
        "!pip -q install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFy9kQ2-SvQ2"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LBoPYp38W9r"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6bzDp4FreS6"
      },
      "outputs": [],
      "source": [
        "###################### Parameters You Can Modify ######################\n",
        "bert_model = \"albert-base-v2\"  # 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2', 'bert-base-uncased', ...\n",
        "freeze_bert = False  # if True, freeze the encoder weights and only update the classification layer weights\n",
        "maxlen = 128  # maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\n",
        "bs = 16  # batch size\n",
        "iters_to_accumulate = 2  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\n",
        "lr = 2e-5  # learning rate\n",
        "epochs = 4  # number of training epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWLGk5Tc8gfL"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm0lAXvTZChm"
      },
      "outputs": [],
      "source": [
        "class SentencePairClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n",
        "        super(SentencePairClassifier, self).__init__()\n",
        "        #  Instantiating BERT-based model object\n",
        "        self.bert_layer = AutoModel.from_pretrained(bert_model)\n",
        "        self.bert_config = AutoConfig.from_pretrained(bert_model)\n",
        "        \n",
        "        hidden_size = self.bert_config.hidden_size\n",
        "        num_labels = 1\n",
        "\n",
        "        # Freeze bert layers and only train the classification layer weights\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        ###################### Your Implementation Here ######################\n",
        "        # Now you need to implement a linear classifier that predicts\n",
        "        # the logits of class labels from the output of the pre-trained\n",
        "        # model \"bert_layer\".\n",
        "        # Note: the inputs of the classifier will have a size of\n",
        "        # \"hidden_size\" and the output size should be \"num_labels\".\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "        ##########################################################################\n",
        "\n",
        "    @autocast()  # run in mixed precision\n",
        "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -input_ids : Tensor containing token ids\n",
        "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
        "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
        "        '''\n",
        "\n",
        "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n",
        "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids, return_dict=False)\n",
        "\n",
        "        logits = None\n",
        "\n",
        "        ###################### Your Implementation Here ######################\n",
        "        # Feeding the output \"pooler_output\" of the pre-trained model to\n",
        "        # your implemented module and saving its output to \"logits\".\n",
        "        logits = self.classifier(pooler_output)\n",
        "\n",
        "        ##########################################################################\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpKx43Iq6znw"
      },
      "source": [
        "## 4. Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0hfGEe2LB9s"
      },
      "outputs": [],
      "source": [
        "# Load the MRPC dataset (train, validation and test)\n",
        "dataset = load_dataset('glue', 'mrpc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpobOzx-Lht7"
      },
      "outputs": [],
      "source": [
        "split = dataset['train'].train_test_split(test_size=0.1, seed=1)  # split the original training data for validation\n",
        "train = split['train']  # 90 % of the original training data\n",
        "val = split['test']   # 10 % of the original training data\n",
        "test = dataset['validation']  # the original validation data is used as test data because the test labels are not available with the datasets library\n",
        "\n",
        "# Transform data into pandas dataframes\n",
        "df_train = pd.DataFrame(train)\n",
        "df_val = pd.DataFrame(val)\n",
        "df_test = pd.DataFrame(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNs2FWNJSLSq"
      },
      "outputs": [],
      "source": [
        "print(df_train.shape)\n",
        "print(df_val.shape)\n",
        "print(df_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irj7itV0UCF_"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWJfh6DV7CB5"
      },
      "source": [
        "## 5. Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc1GQh7yEm4C"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\n",
        "\n",
        "        self.data = data  # pandas dataframe\n",
        "        #Initialize the tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n",
        "\n",
        "        self.maxlen = maxlen\n",
        "        self.with_labels = with_labels \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
        "        sent1 = str(self.data.loc[index, 'sentence1'])\n",
        "        sent2 = str(self.data.loc[index, 'sentence2'])\n",
        "\n",
        "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
        "        encoded_pair = self.tokenizer(sent1, sent2, \n",
        "                        padding='max_length',  # Pad to max_length\n",
        "                        truncation=True,  # Truncate to max_length\n",
        "                        max_length=self.maxlen,  \n",
        "                        return_tensors='pt')  # Return torch.Tensor objects\n",
        "        \n",
        "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
        "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
        "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
        "\n",
        "        if self.with_labels:  # True if the dataset has labels\n",
        "            label = self.data.loc[index, 'label']\n",
        "            return token_ids, attn_masks, token_type_ids, label  \n",
        "        else:\n",
        "            return token_ids, attn_masks, token_type_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SrSNNYTjwe8"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "\n",
        "def evaluate_loss(net, device, criterion, dataloader):\n",
        "    net.eval()\n",
        "\n",
        "    mean_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n",
        "            seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "            logits = net(seq, attn_masks, token_type_ids)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            count += 1\n",
        "\n",
        "    return mean_loss / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl-rhuWsrg01"
      },
      "outputs": [],
      "source": [
        "print(\"Creation of the models' folder...\")\n",
        "!mkdir models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-o6KyaFkU5u"
      },
      "outputs": [],
      "source": [
        "def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
        "\n",
        "    best_loss = np.Inf\n",
        "    best_ep = 1\n",
        "    nb_iterations = len(train_loader)\n",
        "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
        "    iters = []\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n",
        "\n",
        "            # Converting to cuda tensors\n",
        "            seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "    \n",
        "            # Enables autocasting for the forward pass (model + loss)\n",
        "            with autocast():\n",
        "                # Obtaining the logits from the model\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "\n",
        "                # Computing loss\n",
        "                loss = criterion(logits.squeeze(-1), labels.float())\n",
        "                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n",
        "\n",
        "            # Backpropagating the gradients\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (it + 1) % iters_to_accumulate == 0:\n",
        "                # Optimization step\n",
        "                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n",
        "                # otherwise, opti.step() is skipped.\n",
        "                scaler.step(opti)\n",
        "                # Updates the scale for next iteration.\n",
        "                scaler.update()\n",
        "                # Adjust the learning rate based on the number of iterations.\n",
        "                lr_scheduler.step()\n",
        "                # Clear gradients\n",
        "                opti.zero_grad()\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (it + 1) % print_every == 0:  # Print training loss information\n",
        "                print()\n",
        "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
        "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
        "\n",
        "                running_loss = 0.0\n",
        "\n",
        "\n",
        "        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
        "        print()\n",
        "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
        "            print()\n",
        "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
        "            best_loss = val_loss\n",
        "            best_ep = ep + 1\n",
        "\n",
        "    # Saving the model\n",
        "    path_to_model='models/{}_lr_{}_val_loss_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), best_ep)\n",
        "    torch.save(net_copy.state_dict(), path_to_model)\n",
        "    print(\"The model has been saved in {}\".format(path_to_model))\n",
        "\n",
        "    del loss\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_abThXlSr6n"
      },
      "source": [
        "## 6. Training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZWGPomoryxy"
      },
      "outputs": [],
      "source": [
        "#  Set all seeds to make reproducible results\n",
        "set_seed(1)\n",
        "\n",
        "# Creating instances of training and validation set\n",
        "print(\"Reading training data...\")\n",
        "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
        "print(\"Reading validation data...\")\n",
        "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
        "# Creating instances of training and validation dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size=bs, num_workers=5)\n",
        "val_loader = DataLoader(val_set, batch_size=bs, num_workers=5)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
        "print(\"You are using \", device)\n",
        "\n",
        "if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    net = nn.DataParallel(net)\n",
        "\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = AdamW([p for p in net.parameters() if p.requires_grad], lr=lr, weight_decay=1e-2)\n",
        "num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
        "num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
        "t_total = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw2sOrIvCEZz"
      },
      "source": [
        "You can download the model saved in the folder \"models\" by browsing the files on the left of the colab notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDBtVu7JSbUK"
      },
      "source": [
        "## 7. Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAfbt0FjkCfM"
      },
      "outputs": [],
      "source": [
        "print(\"Creation of the results' folder...\")\n",
        "!mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3r8_npVf30D"
      },
      "outputs": [],
      "source": [
        "def get_probs_from_logits(logits):\n",
        "    \"\"\"\n",
        "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    return probs.detach().cpu().numpy()\n",
        "\n",
        "def test_prediction(net, device, dataloader, with_labels=True, result_file=\"results/output.txt\"):\n",
        "    \"\"\"\n",
        "    Predict the probabilities on a dataset with or without labels and print the result in a file\n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    w = open(result_file, 'w')\n",
        "    probs_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if with_labels:\n",
        "            for seq, attn_masks, token_type_ids, _ in tqdm(dataloader):\n",
        "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                probs_all += probs.tolist()\n",
        "        else:\n",
        "            for seq, attn_masks, token_type_ids in tqdm(dataloader):\n",
        "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                probs_all += probs.tolist()\n",
        "\n",
        "    w.writelines(str(prob)+'\\n' for prob in probs_all)\n",
        "    w.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWoWiw6MlPm-"
      },
      "outputs": [],
      "source": [
        "path_to_model = max(glob.glob('/content/models/*'), key=os.path.getctime)\n",
        "\n",
        "path_to_output_file = 'results/output.txt'\n",
        "\n",
        "print(\"Reading test data...\")\n",
        "test_set = CustomDataset(df_test, maxlen, bert_model)\n",
        "test_loader = DataLoader(test_set, batch_size=bs, num_workers=5)\n",
        "\n",
        "model = SentencePairClassifier(bert_model)\n",
        "if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "print()\n",
        "print(\"Loading the weights of the model...\")\n",
        "model.load_state_dict(torch.load(path_to_model))\n",
        "model.to(device)\n",
        "\n",
        "print(\"Predicting on test data...\")\n",
        "test_prediction(net=model, device=device, dataloader=test_loader, with_labels=True,  # set the with_labels parameter to False if your want to get predictions on a dataset without labels\n",
        "                result_file=path_to_output_file)\n",
        "print()\n",
        "print(\"Predictions are available in : {}\".format(path_to_output_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCVAtClcC1qT"
      },
      "source": [
        "You can download the predictions saved in the folder \"results\" by browsing the files on the left of the colab notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywxq1c8DSiV3"
      },
      "source": [
        "## 8. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JYwEPtrlBFX"
      },
      "outputs": [],
      "source": [
        "path_to_output_file = 'results/output.txt'  # path to the file with prediction probabilities\n",
        "\n",
        "labels_test = df_test['label']  # true labels\n",
        "\n",
        "probs_test = pd.read_csv(path_to_output_file, header=None)[0]  # prediction probabilities\n",
        "threshold = 0.5   # you can adjust this threshold for your own dataset\n",
        "preds_test=(probs_test>=threshold).astype('uint8') # predicted labels using the above fixed threshold\n",
        "\n",
        "metric = load_metric(\"glue\", \"mrpc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3ReQL2CUj3I"
      },
      "outputs": [],
      "source": [
        "# Compute the accuracy and F1 scores\n",
        "metric._compute(predictions=preds_test, references=labels_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
